{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I will try to scrape Commons Wikimedia to create a image dataset that can be used to train machine learning algorithms."
      ],
      "metadata": {
        "id": "XhOX8GD8cPfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import"
      ],
      "metadata": {
        "id": "eI6tpaT2WRzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First part is installing and importing all the required libraries. Since selenium and chrome drive are not part of default google colab environment. I will start by installing these. I will use selenium for interacting dynamic elements in the DOM, and after that I will use beautiful soup to extract image links.\n",
        "\n",
        "Along with selenium, requests and beautifulsoup, I will import string and re library which will help with formatting the name, if I don't intend to use the actual name of the file."
      ],
      "metadata": {
        "id": "09Zf1nOn88ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt install chromium-chromium-driver -qq\n",
        "!pip install selenium -qq\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import time\n",
        "import re\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
        "import pandas as pd\n",
        "\n",
        "from urllib.parse import quote\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI2dsn600x5q",
        "outputId": "c1b9d9b2-e6b6-47bd-c59b-01686352f3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package chromium-chromium-driver\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I setup the chrome driver and use Options() to start chrome in headless mode."
      ],
      "metadata": {
        "id": "XeQaNR0O-Uvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Chrome Driver"
      ],
      "metadata": {
        "id": "cowvdqiTWUtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    return webdriver.Chrome(options=chrome_options)"
      ],
      "metadata": {
        "id": "JWWTd8M2-UQp"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The category pages on Wikimedia are sometimes stored in a Tree format, and when the pages are initially loaded, only the parent is visible. This means if we want to scrape the child pages, we will need to use Selenium to interact with the dynamic elements on the page.\n",
        "\n",
        "The expand_all function will find all the Categories that have an expandable subcategory. This function will find the element of class=CategoryTreeToggle CategoryTreeToggleHandlerAttached where attribute aria-expanded=\"false\". It will then click on these elements to expand them one at a time. This function also highlights the expanded elements in yellow. This proved to be quite useful when I was debugging and testing the code in normal mode in chrome driver.\n"
      ],
      "metadata": {
        "id": "kPSv6OyN-hW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expand Collapsible Section"
      ],
      "metadata": {
        "id": "CFnDoabXWXSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_all(driver):\n",
        "    # Find all elements where class=CategoryTreeToggle CategoryTreeToggleHandlerAttached and aria-expanded=\"false\"\n",
        "    buttons = driver.find_elements(By.CSS_SELECTOR, 'a.CategoryTreeToggle.CategoryTreeToggleHandlerAttached[aria-expanded=\"false\"]')\n",
        "\n",
        "    print(\"Buttons found:\", len(buttons))\n",
        "\n",
        "    # Loop through each button and click it\n",
        "    for button in buttons:\n",
        "        parent_div = button.find_element(By.XPATH, './ancestor::div')\n",
        "\n",
        "        # Highlight the parent div by changing its style # Useful while developing and tesing\n",
        "        driver.execute_script(\"arguments[0].style.backgroundColor = 'yellow'; arguments[0].style.color = 'blue';\", parent_div)\n",
        "\n",
        "        # Click on the button to expand\n",
        "        button.click()\n",
        "\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "_l9qGmIJ_tWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible that there are many subcategories in a page, so I created a function check_remaining_buttons which will call itself recursively and keep executing expand_all if check_remaining_buttons returns anything other 0."
      ],
      "metadata": {
        "id": "upLbl326_v-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expand all Sections"
      ],
      "metadata": {
        "id": "nIbcwApsWdLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_remaining_buttons(driver):\n",
        "    # Check if there are any more expandable buttons\n",
        "    remaining_buttons = driver.find_elements(By.CSS_SELECTOR, 'a.CategoryTreeToggle.CategoryTreeToggleHandlerAttached[aria-expanded=\"false\"]')\n",
        "\n",
        "    print(\"Remaining buttons:\", len(remaining_buttons))\n",
        "\n",
        "    # If there are remaining buttons, expand them\n",
        "    if remaining_buttons: # if remaining_buttons, the check condition becomes false\n",
        "        expand_all(driver)\n",
        "        check_remaining_buttons(driver)"
      ],
      "metadata": {
        "id": "QQkXuSU8_vsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once all the buttons are expanded the links to the Category pages can be collected. The simplest way to do this will be finding all the divs of class=CategoryTreeItem and finding the hrefs withing them. If these contain the term \"wiki/Category:\", they will be stored in category_links. All the hrefs are then added to a list."
      ],
      "metadata": {
        "id": "mwVL4K12ANtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Page Links"
      ],
      "metadata": {
        "id": "Ru4hlAGGWfjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_category_links(driver):\n",
        "    all_links = []\n",
        "    all_links.append(driver.current_url.strip(','))\n",
        "    category_links = driver.find_elements(By.CSS_SELECTOR, 'div.CategoryTreeItem a[href*=\"wiki/Category:\"]')\n",
        "\n",
        "    print(\"Category links found:\", len(category_links))\n",
        "\n",
        "    # Print out the href of each category link\n",
        "    for link in category_links:\n",
        "        all_links.append(link.get_attribute('href'))\n",
        "        #print(link.get_attribute('href'))\n",
        "\n",
        "    return all_links"
      ],
      "metadata": {
        "id": "gCqkyqt0AM15"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 1"
      ],
      "metadata": {
        "id": "6_PAquSmWisp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see this in action.\n",
        "\n",
        "Suppose I am building a dataset to train a model that will try to identify the maker of aircraft in the photo. I will need to find thousands of examples for each model to have any hope of training the model. The code that I wrote allows me to go through all the the category pages and get all the links to category pages related to a particular aircraft.\n",
        "\n",
        "I will use the example of Airbus A300 here. I searched for a A300 photo on wikimedia and through category pages at bottom I navigated to the page \"Airbus_A300_by_variant\" which is higher in the tree. This page should contain links to all of the A300 pages. When I run the code on this page I get close to 900 pages in return. This would have been quite a task to do manually, and it would be prone to a lot of errors if done manually."
      ],
      "metadata": {
        "id": "qjO3Mk8TBdb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver = setup_driver()\n",
        "driver.get(\"https://commons.wikimedia.org/wiki/Category:Airbus_A300_by_variant\")\n",
        "check_remaining_buttons(driver)\n",
        "all_links = find_category_links(driver)\n",
        "driver.quit()\n",
        "\n",
        "print(\"Links found:\", len(all_links))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cRPkptIA2-Q",
        "outputId": "09a251ef-ee08-4b9d-83b4-defb5fd76c53"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining buttons: 22\n",
            "Buttons found: 22\n",
            "Remaining buttons: 21\n",
            "Buttons found: 21\n",
            "Remaining buttons: 8\n",
            "Buttons found: 8\n",
            "Remaining buttons: 2\n",
            "Buttons found: 2\n",
            "Remaining buttons: 0\n",
            "Category links found: 887\n",
            "Links found: 888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_links[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2OuW2ACBbUW",
        "outputId": "6409d7db-0071-4f6a-e80c-0ed0ad446c45"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://commons.wikimedia.org/wiki/Category:Airbus_A300_by_variant',\n",
              " 'https://commons.wikimedia.org/wiki/Category:Airbus_A300B1',\n",
              " 'https://commons.wikimedia.org/wiki/Category:F-OCAZ_(aircraft)',\n",
              " 'https://commons.wikimedia.org/wiki/Category:F-WUAB_(Airbus_A300B1)',\n",
              " 'https://commons.wikimedia.org/wiki/Category:Airbus_A300_maiden_flight']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explain the next part I will choose a page that doesn't have too many children. I choose the page for a particular aircraft which is registered F-BUAD"
      ],
      "metadata": {
        "id": "0i_N5a2tWAkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Open the desired webpage\n",
        "    driver.get('https://commons.wikimedia.org/wiki/Category:F-BUAD_(aircraft)')\n",
        "\n",
        "    # Start the process by checking and expanding all buttons\n",
        "    check_remaining_buttons(driver)\n",
        "\n",
        "    # After expanding all buttons, find the category links\n",
        "    all_links = find_category_links(driver)\n",
        "\n",
        "    # Close the driver after completion\n",
        "    driver.quit()\n",
        "\n",
        "    return all_links\n",
        "\n",
        "driver = setup_driver()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    all_links = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hfTkw1C0GQL",
        "outputId": "bec22ee3-e06d-4257-f452-6b0c69d4548f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining buttons: 1\n",
            "Buttons found: 1\n",
            "Remaining buttons: 0\n",
            "Category links found: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_links[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-rQz-ZBdO_H",
        "outputId": "e018e638-0531-49b2-ba1b-40a01ab76312"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://commons.wikimedia.org/wiki/Category:F-BUAD_(aircraft)',\n",
              " 'https://commons.wikimedia.org/wiki/Category:Airbus_A300_ZERO-G',\n",
              " 'https://commons.wikimedia.org/wiki/Category:COVID-19_vaccination_in_Airbus_Zero_Gravity',\n",
              " 'https://commons.wikimedia.org/wiki/Category:Airbus_A300_ZERO-G_at_Paris_Air_Show_2009']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Image URLs and Test 2"
      ],
      "metadata": {
        "id": "7U2PRySZWqbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 4 category pages that I need to scrape. I wrote a code that will navigate to individual pages of images on these pages. This can be done by finding all hrefs that are from class=galleryfilename galleryfilename-truncate. I will visit each of these pages and get the hrefs from class=internal. This will give me the highest resolution images to work with.\n",
        "\n",
        "I also printed the current category page name and the number of image links the code found on that page. PAGES shows the image page found on each category and TOTAL IMGURLS shows the total images found so far."
      ],
      "metadata": {
        "id": "bjiB81vsD8tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store image URLs and page links\n",
        "l_t_p = []  # Stores page links for images\n",
        "img_urls = []  # Stores final image URLs\n",
        "\n",
        "# Loop through all the URLs (which are in `all_links`)\n",
        "for url in all_links:\n",
        "    # Print the current category or page being processed (removes \"https://commons.wikimedia.org/wiki/Category:\")\n",
        "    categoryname = url.replace(\"https://commons.wikimedia.org/wiki/Category:\", \"\")\n",
        "    print(f\"CATEGORY: {categoryname}\")\n",
        "\n",
        "    # Fetch the source code of the page\n",
        "    source_code = requests.get(url, allow_redirects=False)\n",
        "\n",
        "    # Encode the source code in ASCII to handle any non-ASCII characters (replace them if needed)\n",
        "    plain_text = source_code.text.encode('ascii', 'replace')\n",
        "\n",
        "    # Parse the page content using BeautifulSoup\n",
        "    soup = BeautifulSoup(plain_text, 'html.parser')\n",
        "\n",
        "    # Find all links for images in the current category page\n",
        "    for link in soup.findAll('a', {'class': 'galleryfilename galleryfilename-truncate'}):\n",
        "        # Get the link for each image page\n",
        "        image_link = 'https://en.wikipedia.org/' + link.get('href')\n",
        "\n",
        "        # Append the image page link to the list\n",
        "        l_t_p.append(image_link)\n",
        "\n",
        "    # Iterate over all image page links collected in l_t_p\n",
        "    for l_t_p1 in l_t_p:\n",
        "        # Now fetch the image page (not the category page)\n",
        "        url = l_t_p1\n",
        "        source_code = requests.get(url, allow_redirects=False)\n",
        "\n",
        "        # Encode the source code in ASCII to handle any non-ASCII characters (replace them if needed)\n",
        "        plain_text = source_code.text.encode('ascii', 'replace')\n",
        "\n",
        "        # Parse the image page content using BeautifulSoup\n",
        "        soup = BeautifulSoup(plain_text, 'html.parser')\n",
        "\n",
        "        # Find all links for the actual image URLs on the image page\n",
        "        for l_t_p2 in soup.findAll('a', {'class': 'internal'}):\n",
        "            # Get the href attribute for the image link\n",
        "            href = l_t_p2.get('href')\n",
        "\n",
        "            # Construct the full image URL\n",
        "            img_url = 'https:' + str(href)\n",
        "\n",
        "            # Print the image URL (can be used for debugging or confirmation)\n",
        "            # print(img_url)\n",
        "\n",
        "            # Append the image URL to the list\n",
        "            img_urls.append(img_url)\n",
        "\n",
        "    # Print summary for each category page processed\n",
        "    print(f\"IMAGE PAGES FOR THIS CATEGORY:{len(l_t_p)}\")  # Shows how many image pages were processed\n",
        "    print()\n",
        "    l_t_p = []  # Clear the list for the next category page\n",
        "\n",
        "print(f\"TOTAL IMGURLS:{len(img_urls)}\")  # Shows how many image URLs have been found\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxvHCrzV__ob",
        "outputId": "66ba64b0-df13-4166-a9eb-fe018f03771f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CATEGORY: F-BUAD_(aircraft)\n",
            "IMAGE PAGES FOR THIS CATEGORY:5\n",
            "\n",
            "CATEGORY: Airbus_A300_ZERO-G\n",
            "IMAGE PAGES FOR THIS CATEGORY:15\n",
            "\n",
            "CATEGORY: COVID-19_vaccination_in_Airbus_Zero_Gravity\n",
            "IMAGE PAGES FOR THIS CATEGORY:19\n",
            "\n",
            "CATEGORY: Airbus_A300_ZERO-G_at_Paris_Air_Show_2009\n",
            "IMAGE PAGES FOR THIS CATEGORY:6\n",
            "\n",
            "TOTAL IMGURLS:45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Duplicate Images"
      ],
      "metadata": {
        "id": "IjMRzAVNWxSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use the set() function in python to remove any duplicate pages. This can happen if I start from a page that is very high in the tree."
      ],
      "metadata": {
        "id": "RnEYToH3EteO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_img_urls = list(set(img_urls))\n",
        "print(f\"Duplicate Links:{len(img_urls)-len(unique_img_urls)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANkV3SxI6JVx",
        "outputId": "5418c7be-cbcf-42ba-e3cd-d90acb53bf84"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate Links:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_img_urls[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwCt4sMdmyEw",
        "outputId": "0625ef08-c292-406c-a29b-70d0db763dc8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://upload.wikimedia.org/wikipedia/commons/d/d5/Parabelflug_PB261027.jpg',\n",
              " 'https://upload.wikimedia.org/wikipedia/commons/6/6b/2021-12-21-Impfung_Zero_Gravity-6767.jpg',\n",
              " 'https://upload.wikimedia.org/wikipedia/commons/1/1b/A300_Zero_G_crop.jpg',\n",
              " 'https://upload.wikimedia.org/wikipedia/commons/b/be/Salon_du_Bourget_20090619_122.jpg',\n",
              " 'https://upload.wikimedia.org/wikipedia/commons/8/80/2021-12-13-Impfung_Zero_Gravity-6514.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Download Images Using Python"
      ],
      "metadata": {
        "id": "kmefv2d7W2p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the images can be simply downloaded to a specified directory."
      ],
      "metadata": {
        "id": "qAu_vB5fE7hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_image(url, prefix, save_dir):\n",
        "    # Extract the filename from the URL\n",
        "    filename = url.split('/')[-1]\n",
        "\n",
        "    # Add the prefix to the filename\n",
        "    new_filename = prefix + filename\n",
        "\n",
        "    # Create the full path to save the image\n",
        "    file_path = os.path.join(save_dir, new_filename)\n",
        "\n",
        "    # Send an HTTP GET request to fetch the image\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Open the file in binary write mode and save the image\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded: {new_filename}\")\n",
        "    else:\n",
        "        print(f\"Failed to download: {url}\")\n",
        "\n",
        "prefix = \"Airbus A300 \"\n",
        "save_dir = \"/Users/[YOURUSERNAME]/Downloads/TRAIN/RAW/Airbus_A300\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "for url in img_urls:\n",
        "    download_image(url, prefix, save_dir)\n"
      ],
      "metadata": {
        "id": "cgDFhXao5XjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: Use Jdownloader to download"
      ],
      "metadata": {
        "id": "3hgMm0s9W9X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, I usually use a batch download utility like Jdownloader to download a large set of images. It has a robust feature set, and allows pausing and resuming downloads.\n",
        "\n",
        "However unlike previous code where adding a prefix to filename is very easy, using jdownloader means I need to use a different technique. Jdownloader has a great feature where embedding #filename=name at the end of url will set the filename to \"name\", or whatever we choose it to be.\n",
        "\n",
        "I will use this ability to extract filename from the img urls using regex and add a prefix to a filename this way. Since urls cannot contain space character, any spaces in prefix or filename will be written as \"%20\" in url. Jdownloader can understand this representation and will substitute %20 with space when downloading the file."
      ],
      "metadata": {
        "id": "Cs4Mcw7TFFYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the new URL\n",
        "def generate_new_url(url):\n",
        "    # Extract the part of the URL after the last '/'\n",
        "    filename = url.split('/')[-1]\n",
        "\n",
        "    # Replace spaces in the filename with '%20'\n",
        "    prefix_nospace = prefix.replace(\" \", \"%20\")\n",
        "\n",
        "    # Construct the new URL\n",
        "    new_url = url + \"#filename=\" + prefix_nospace + filename\n",
        "\n",
        "    return new_url\n",
        "\n",
        "prefix = \"Airbus A300 \"\n",
        "\n",
        "new_img_urls = [generate_new_url(url) for url in unique_img_urls]\n",
        "\n",
        "# Print the new URLs\n",
        "for idx, new_url in enumerate(new_img_urls):\n",
        "    print(new_url)\n",
        "    if idx == 4:\n",
        "      break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar5Rb2tt3-uF",
        "outputId": "62ccb78d-2db2-430f-f31d-30cabcb93505"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://upload.wikimedia.org/wikipedia/commons/d/d5/Parabelflug_PB261027.jpg#filename=Airbus%20A300%20Parabelflug_PB261027.jpg\n",
            "https://upload.wikimedia.org/wikipedia/commons/6/6b/2021-12-21-Impfung_Zero_Gravity-6767.jpg#filename=Airbus%20A300%202021-12-21-Impfung_Zero_Gravity-6767.jpg\n",
            "https://upload.wikimedia.org/wikipedia/commons/1/1b/A300_Zero_G_crop.jpg#filename=Airbus%20A300%20A300_Zero_G_crop.jpg\n",
            "https://upload.wikimedia.org/wikipedia/commons/b/be/Salon_du_Bourget_20090619_122.jpg#filename=Airbus%20A300%20Salon_du_Bourget_20090619_122.jpg\n",
            "https://upload.wikimedia.org/wikipedia/commons/8/80/2021-12-13-Impfung_Zero_Gravity-6514.jpg#filename=Airbus%20A300%202021-12-13-Impfung_Zero_Gravity-6514.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all that is left is to make all the images of same resolution and if needed apply transformation like rotation, horizontal or vertical mirroring to a small percentage of images. Such transformation can help a image recognition model generalise better."
      ],
      "metadata": {
        "id": "LNYyZKzUGo1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform Image"
      ],
      "metadata": {
        "id": "v7gskoxxXCvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image_path, output_path, target_width, target_height, flip_type=None, rotate_degrees=None, flip_percent=0, rotate_percent=0):\n",
        "    \"\"\"\n",
        "    Crop, resize, flip, and rotate image based on the given parameters.\n",
        "\n",
        "    Parameters:\n",
        "        image_path (str): Path to the input image.\n",
        "        output_path (str): Path to save the processed image.\n",
        "        target_width (int): Target width for the crop.\n",
        "        target_height (int): Target height for the crop.\n",
        "        flip_type (str): 'horizontal' or 'vertical' to flip the image (default None).\n",
        "        rotate_degrees (bool): True to rotate (if flip is None), False to not rotate (default None).\n",
        "        flip_percent (int): The probability (in %) of flipping the image.\n",
        "        rotate_percent (int): The probability (in %) of rotating the image.\n",
        "    \"\"\"\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "\n",
        "        # Crop the image to the target resolution (centered crop)\n",
        "        left = (img_width - target_width) / 2\n",
        "        top = (img_height - target_height) / 2\n",
        "        right = (img_width + target_width) / 2\n",
        "        bottom = (img_height + target_height) / 2\n",
        "\n",
        "        img = img.crop((left, top, right, bottom))\n",
        "\n",
        "        # Resize the image to the target resolution\n",
        "        img = img.resize((target_width, target_height))\n",
        "\n",
        "        # Flip the image with a certain probability (flip_percent)\n",
        "        if flip_percent > 0 and random.random() < flip_percent / 100:\n",
        "            if flip_type == 'horizontal':\n",
        "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            elif flip_type == 'vertical':\n",
        "                img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "\n",
        "        # Rotate the image with a certain probability (rotate_percent)\n",
        "        if rotate_percent > 0 and random.random() < rotate_percent / 100:\n",
        "            if rotate_degrees is None:  # If rotate_degrees is None, pick randomly from [90, 180, 270]\n",
        "                rotate_degrees = random.choice([90, 180, 270])\n",
        "            img = img.rotate(rotate_degrees)\n",
        "\n",
        "        # Save the processed image\n",
        "        img.save(output_path)\n",
        "        print(f\"Processed image saved to: {output_path}\")\n",
        "\n",
        "def process_directory(input_dir, output_dir, target_width, target_height, flip_type=None, rotate_degrees=None, flip_percent=0, rotate_percent=0):\n",
        "    \"\"\"\n",
        "    Process all images in the input directory.\n",
        "\n",
        "    Parameters:\n",
        "        input_dir (str): Directory containing input images.\n",
        "        output_dir (str): Directory to save processed images.\n",
        "        target_width (int): Target width for cropping and resizing.\n",
        "        target_height (int): Target height for cropping and resizing.\n",
        "        flip_type (str): 'horizontal' or 'vertical' to flip images (default None).\n",
        "        rotate_degrees (int): Rotation degrees (90, 180, or 270) (default None).\n",
        "        flip_percent (int): The probability (in %) of flipping images.\n",
        "        rotate_percent (int): The probability (in %) of rotating images.\n",
        "    \"\"\"\n",
        "    # Get all image files in the directory\n",
        "    image_files = glob.glob(os.path.join(input_dir, '*.*'))\n",
        "\n",
        "    # Supported image formats (optional, you can add more)\n",
        "    valid_formats = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
        "\n",
        "    # Filter the list to include only valid image files\n",
        "    image_files = [file for file in image_files if os.path.splitext(file)[1].lower() in valid_formats]\n",
        "\n",
        "    # Process each image\n",
        "    for image_path in image_files:\n",
        "        # Prepare output path\n",
        "        output_image_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "\n",
        "        # Process the image\n",
        "        process_image(image_path, output_image_path, target_width, target_height, flip_type, rotate_degrees, flip_percent, rotate_percent)\n",
        "\n",
        "# Example usage\n",
        "input_dir = '/Users/[YOURUSERNAME]/Downloads/TRAIN/RAW/Airbus_A300'\n",
        "output_dir = '/Users/[YOURUSERNAME]/Downloads/TRAIN/TRANSFORMED/Airbus_A300'\n",
        "target_width = 800\n",
        "target_height = 600\n",
        "flip_type = 'horizontal'\n",
        "rotate_degrees = None\n",
        "flip_percent = 10\n",
        "rotate_percent = 15\n",
        "\n",
        "process_directory(input_dir, output_dir, target_width, target_height, flip_type, rotate_degrees, flip_percent, rotate_percent)\n"
      ],
      "metadata": {
        "id": "eXwJKzuE4mo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it. Now to build a image dataset, I need to just give a url, and the code will do the rest."
      ],
      "metadata": {
        "id": "kpSPph0aHG4o"
      }
    }
  ]
}
